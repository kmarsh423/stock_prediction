{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times \n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a,b):\n",
    "    # shufflemtwo arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "    \n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                 test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe.\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for intance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 ( that is 50+10)\n",
    "    # this last sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"] = X[train_samples:]\n",
    "        result[\"y_test\"] = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:\n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                    test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets and convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features,units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers -1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units,return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 4\n",
    "# whether to scale feature columns and output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "#features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% droput\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 50\n",
    "#TSLA stock market\n",
    "ticker = \"TSLA\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they do not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "229/229 [==============================] - 23s 91ms/step - loss: 0.0032 - mean_absolute_error: 0.0362 - val_loss: 3.9108e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00039, saving model to results\\2022-01-17_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-4-layers-2-units-256.h5\n",
      "Epoch 2/50\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 7.6622e-04 - mean_absolute_error: 0.0188 - val_loss: 2.0910e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00039 to 0.00021, saving model to results\\2022-01-17_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-4-layers-2-units-256.h5\n",
      "Epoch 3/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 5.7901e-04 - mean_absolute_error: 0.0169 - val_loss: 3.5251e-04 - val_mean_absolute_error: 0.0189\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00021\n",
      "Epoch 4/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 6.5886e-04 - mean_absolute_error: 0.0180 - val_loss: 4.2627e-04 - val_mean_absolute_error: 0.0145\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00021\n",
      "Epoch 5/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 6.5620e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3017e-04 - val_mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00021\n",
      "Epoch 6/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 6.9090e-04 - mean_absolute_error: 0.0184 - val_loss: 2.8828e-04 - val_mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00021\n",
      "Epoch 7/50\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 4.8182e-04 - mean_absolute_error: 0.0152 - val_loss: 2.3263e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00021\n",
      "Epoch 8/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 5.1589e-04 - mean_absolute_error: 0.0152 - val_loss: 2.4994e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00021\n",
      "Epoch 9/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 5.3033e-04 - mean_absolute_error: 0.0165 - val_loss: 2.4435e-04 - val_mean_absolute_error: 0.0122\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00021\n",
      "Epoch 10/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 5.9826e-04 - mean_absolute_error: 0.0174 - val_loss: 2.7581e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00021\n",
      "Epoch 11/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 4.4550e-04 - mean_absolute_error: 0.0143 - val_loss: 2.1625e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00021\n",
      "Epoch 12/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 5.6501e-04 - mean_absolute_error: 0.0166 - val_loss: 2.1387e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00021\n",
      "Epoch 13/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 6.1110e-04 - mean_absolute_error: 0.0174 - val_loss: 2.2186e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00021\n",
      "Epoch 14/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 4.8889e-04 - mean_absolute_error: 0.0162 - val_loss: 2.5987e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00021\n",
      "Epoch 15/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 4.8726e-04 - mean_absolute_error: 0.0161 - val_loss: 2.3361e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00021\n",
      "Epoch 16/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.0734e-04 - mean_absolute_error: 0.0147 - val_loss: 3.6027e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00021\n",
      "Epoch 17/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.6049e-04 - mean_absolute_error: 0.0161 - val_loss: 3.3867e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00021\n",
      "Epoch 18/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.6957e-04 - mean_absolute_error: 0.0152 - val_loss: 2.3640e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00021\n",
      "Epoch 19/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 5.3589e-04 - mean_absolute_error: 0.0165 - val_loss: 2.2094e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00021\n",
      "Epoch 20/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.5465e-04 - mean_absolute_error: 0.0158 - val_loss: 2.2614e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00021\n",
      "Epoch 21/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 4.7413e-04 - mean_absolute_error: 0.0154 - val_loss: 2.0018e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00021 to 0.00020, saving model to results\\2022-01-17_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-4-layers-2-units-256.h5\n",
      "Epoch 22/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.5996e-04 - mean_absolute_error: 0.0159 - val_loss: 3.4286e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00020\n",
      "Epoch 23/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.1019e-04 - mean_absolute_error: 0.0150 - val_loss: 1.8387e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00020 to 0.00018, saving model to results\\2022-01-17_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-4-layers-2-units-256.h5\n",
      "Epoch 24/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.4783e-04 - mean_absolute_error: 0.0154 - val_loss: 2.0916e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00018\n",
      "Epoch 25/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 5.3829e-04 - mean_absolute_error: 0.0170 - val_loss: 2.0023e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00018\n",
      "Epoch 26/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.7687e-04 - mean_absolute_error: 0.0148 - val_loss: 2.0482e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00018\n",
      "Epoch 27/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.3098e-04 - mean_absolute_error: 0.0157 - val_loss: 3.7443e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00018\n",
      "Epoch 28/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 4.7801e-04 - mean_absolute_error: 0.0163 - val_loss: 2.2436e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00018\n",
      "Epoch 29/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 4.3877e-04 - mean_absolute_error: 0.0156 - val_loss: 2.2204e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00018\n",
      "Epoch 30/50\n",
      "229/229 [==============================] - 17s 75ms/step - loss: 3.7208e-04 - mean_absolute_error: 0.0146 - val_loss: 1.8880e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00018\n",
      "Epoch 31/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.7635e-04 - mean_absolute_error: 0.0150 - val_loss: 1.5761e-04 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00018 to 0.00016, saving model to results\\2022-01-17_TSLA-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-4-layers-2-units-256.h5\n",
      "Epoch 32/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.6732e-04 - mean_absolute_error: 0.0152 - val_loss: 2.0641e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00016\n",
      "Epoch 33/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.2679e-04 - mean_absolute_error: 0.0156 - val_loss: 2.1219e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00016\n",
      "Epoch 34/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.8496e-04 - mean_absolute_error: 0.0144 - val_loss: 2.2999e-04 - val_mean_absolute_error: 0.0094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00016\n",
      "Epoch 35/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.0065e-04 - mean_absolute_error: 0.0160 - val_loss: 3.3898e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00016\n",
      "Epoch 36/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.6244e-04 - mean_absolute_error: 0.0160 - val_loss: 1.7832e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00016\n",
      "Epoch 37/50\n",
      "229/229 [==============================] - 17s 73ms/step - loss: 3.6657e-04 - mean_absolute_error: 0.0150 - val_loss: 1.8926e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00016\n",
      "Epoch 38/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.4403e-04 - mean_absolute_error: 0.0155 - val_loss: 3.6674e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00016\n",
      "Epoch 39/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.7661e-04 - mean_absolute_error: 0.0160 - val_loss: 1.8616e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00016\n",
      "Epoch 40/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.6789e-04 - mean_absolute_error: 0.0150 - val_loss: 2.0540e-04 - val_mean_absolute_error: 0.0132\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00016\n",
      "Epoch 41/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.6692e-04 - mean_absolute_error: 0.0167 - val_loss: 1.7424e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00016\n",
      "Epoch 42/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.8352e-04 - mean_absolute_error: 0.0152 - val_loss: 1.6385e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00016\n",
      "Epoch 43/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.8710e-04 - mean_absolute_error: 0.0146 - val_loss: 2.2590e-04 - val_mean_absolute_error: 0.0069\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00016\n",
      "Epoch 44/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 3.4785e-04 - mean_absolute_error: 0.0144 - val_loss: 1.6629e-04 - val_mean_absolute_error: 0.0065\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00016\n",
      "Epoch 45/50\n",
      "229/229 [==============================] - 17s 74ms/step - loss: 4.5430e-04 - mean_absolute_error: 0.0161 - val_loss: 1.9463e-04 - val_mean_absolute_error: 0.0067\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00016\n",
      "Epoch 46/50\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 4.0634e-04 - mean_absolute_error: 0.0148 - val_loss: 3.0888e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00016\n",
      "Epoch 47/50\n",
      "229/229 [==============================] - 18s 77ms/step - loss: 3.7609e-04 - mean_absolute_error: 0.0147 - val_loss: 2.0035e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00016\n",
      "Epoch 48/50\n",
      "229/229 [==============================] - 18s 79ms/step - loss: 2.9907e-04 - mean_absolute_error: 0.0137 - val_loss: 1.8821e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00016\n",
      "Epoch 49/50\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 3.5762e-04 - mean_absolute_error: 0.0148 - val_loss: 3.1378e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00016\n",
      "Epoch 50/50\n",
      "229/229 [==============================] - 17s 76ms/step - loss: 4.9130e-04 - mean_absolute_error: 0.0150 - val_loss: 1.8905e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00016\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE,\n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                     dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see\n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"], batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(data[\"X_test\"],data[\"y_test\"]), callbacks=[checkpointer, tensorboard], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.title(\"Tesla Price Over Time\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 4 days is $1049.76\n",
      "huber_loss loss: 0.00015760901442263275\n",
      "Mean Absolute Error: 11.391240163247232\n",
      "Mean Absolute Error Percentage: 9.67478%\n",
      "Accuracy score: 0.5506993006993007\n",
      "Total buy profit: 806.0838260650635\n",
      "Total sell profit: -682.9910764694214\n",
      "Total profit: 123.09274959564209\n",
      "Profit per trade: 0.21519711467769595\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is ${future_price:.2f}\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Mean Absolute Error Percentage:\", str(round((mean_absolute_error / final_df[\"adjclose\"].mean())*100, 5)) + '%')\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6wklEQVR4nO3deXwU9f348dc79wkJp9yg4BHkEOPRFlFEDm+t+gNtK1atotV61Nartdqq9Wtb61UP6n2heGO9UERBFJVL5CbckZCEkJA72c2+f3/MZNmEXCS72Rzv5+Oxj539zGdm3rOBee/nMzOfEVXFGGOMaUhEuAMwxhjT9lmyMMYY0yhLFsYYYxplycIYY0yjLFkYY4xplCULY4wxjbJkYdo9EflcRC5vhe0MFJFiEYkM9bbaChE5QUTWhzsOE36WLEyrcw+41S+fiJQFfP5FK8eiIlLibvtHEXmgvmSgqttVNUlVq4Icg4jIH0Rko/tdbBeR+0QkNpjbqWfbvwj47svcv4f/76OqC1X1sFDHYdo+Sxam1bkH3CRVTQK2A2cGlL0chpBGubFMAC4CflO7gohEhXD7DwNXABcDycCpwMnA7GBvqPZ+qOrLAX+LU4Gdtf4+xgCWLEwbIiIRInKLiGwSkTwRmS0i3dx5cSLyklteICLfiUjvOtZxiIh85tbbLSIvi0hKU7avquuAhcCRIjLYbXVcJiLbgc8CyqLcbXUTkWdFZKeI5IvIOwFxnCEiK9xYvxKRkfXs8zDgauAXqvq1qnpVdTVwHjBFRE4WkeNFZFdgi0dEzhWRlU343vbbj6Z8FwHbOUlEMgM+b3VbQSvdFtnTItJbRD4UkSIR+VREUgPqH+/uf4GIfC8iJx3I9k3bYcnCtCW/A84BTgT6AvnAf9x504GuwACgOzADKKtjHQL83V3+CLf+nU3ZuIikAScAywOKT3TXM7mORV4EEoDhQC/g3+56xgDPAFe6sT4JzKmnW2kCkKmq3wYWquoOYDEwUVUXAyU4rY1qFwGvuNMNfW9N2Y8DdR4wETgUOBP4ELgN6IFzTPkdgIj0A94H7ga6ATcBb4pIzyDEYFqZJQvTllwJ3K6qmapagXOQP9/9Je/BOfAOVdUqVV2qqoW1V6CqGar6iapWqGou8ADOgbIhy0QkH3gPeAp4NmDenapaoqo1EpOI9MHptpmhqvmq6lHVL9zZvwGeVNVv3FifByqA4+vYdg8gq564stz5ALOAC91tJwOnuWXQ8PfW4H400yOqmq2qP+K0xL5R1eXutt8GjnLr/RL4QFU/UFWfqn4CLHFjN+1MKPthjTlQg4C3RcQXUFYF9Mb5FT8AeNXtVnoJ5wDpCVyBiPTCOQdwAk7/fwTOL+2GjFHVjFrrqZ7cUc8yA4A9qlrXugcB00Xk2oCyGJxf/bXtBvrUs40+wBZ3+hXgKxG5Cvg5sExVtwVsr77vrbH9aI7sgOmyOj5Xn+sYBFwgImcGzI8G5gcxFtNKrGVh2pIdwKmqmhLwilPVH91f7nepahrwU+AMnBPCtf0dUGCkqnbB+XUrddRrqvqGZd4BdKvnfMgO4J5a+5GgqrPqqPsZMEBEjg0sFJEBOC2ReQCqugbYhtOaCeyCqt5end9bE/YjlHYAL9aKK1FV7wtDLKaFLFmYtuQJ4B4RGQQgIj1F5Gx3eryIjHBP8hbidEvVdQlrMlAMFLh95n8IRaCqmoXTV/+YiKSKSLSIjHNn/xeYISLHuZfFJorI6W73Ue31bMDZ75fdk8GRIjIceBP4VFU/Daj+Cs75gHHA6wHl9X5vYfYScKaITHb3K849Yd4/3IGZA2fJwrQlDwFzgLkiUoRzgvc4d95BwBs4iWIt8AXOwai2u4AxwF6ck6tvhTDeX+EkrXVADnA9gKouwTlv8ShOF1gGcEkD67kG51zJSziJ7iPgc5wTyYFmAScBn6nq7oDyhr63sHFP0p+Nc/I7F6el8QfsuNMuiT38yBhjTGMswxtjjGmUJQtjjDGNsmRhjDGmUZYsjDHGNKrD3pTXo0cPHTx4cLjDMMaYdmXp0qW7VXW/IVk6bLIYPHgwS5YsCXcYxhjTrojItrrKrRvKGGNMoyxZGGOMaZQlC2OMMY3qsOcs6uLxeMjMzKS8vDzcoZgmiouLo3///kRHR4c7FGM6tU6VLDIzM0lOTmbw4MGBQ1CbNkpVycvLIzMzkyFDhoQ7HGM6tU7VDVVeXk737t0tUbQTIkL37t2tJWhMGxCyZCEiz4hIjoisCij7h4isc5/f+3bgswBE5FYRyRCR9SIyOaD8aBH5wZ33sLTwSG+Jon2xv5cxbUMoWxbPAVNqlX0CHKmqI4ENwK3gf/bxNJxnGU/BeUZA9cPpHweuAIa5r9rrNMaYTsfjgWeeAZ+v8brBELJkoaoLgD21yuaqqtf9uBiofgjK2cCr7nOTt+CM/3+s+5zjLqr6tTpjqb+A82D6du3tt99GRFi3bl2jdR988EFKS0ubva3nnnuOa665ps7ynj17Mnr0aNLS0vjvf/9b5/Jz5szhvvvswWbGtDX//jdcdhk891zrbC+c5ywuxXnSGEA/aj4jONMt6+dO1y5v12bNmsXYsWN59dVXG63b0mTRkKlTp7JixQo+//xzbrvtNrKzs2vM93q9nHXWWdxyyy0h2b4xpvlyc5333bsbrhcsYUkWInI74AVeri6qo5o2UF7feq8QkSUisiS3+ptsY4qLi1m0aBFPP/10jWRRVVXFTTfdxIgRIxg5ciSPPPIIDz/8MDt37mT8+PGMHz8egKSkJP8yb7zxBpdccgkA7733HscddxxHHXUUp5xyyn4H/ob06tWLQw45hG3btnHJJZdw4403Mn78eG6++eYaLZPs7GzOPfdcRo0axahRo/jqq68AeOmllzj22GMZPXo0V155JVVVdT3t1BgTTBHu0bu1uqFa/dJZEZkOnAFM0H2P6csEBgRU6w/sdMv711FeJ1WdCcwESE9Pb/ARgNdfDytWHGDwjRg9Gh58sOE677zzDlOmTOHQQw+lW7duLFu2jDFjxjBz5ky2bNnC8uXLiYqKYs+ePXTr1o0HHniA+fPn06NHjwbXO3bsWBYvXoyI8NRTT3H//ffzr3/9q0lxb968mc2bNzN06FAANmzYwKeffkpkZCTPBbRxf/e733HiiSfy9ttvU1VVRXFxMWvXruW1115j0aJFREdHc/XVV/Pyyy9z8cUXN2nbxpjm6dDJQkSmADcDJ6pqYN/KHOAVEXkA6ItzIvtbVa0SkSIROR74BrgYeKQ1Yw62WbNmcf311wMwbdo0Zs2axZgxY/j000+ZMWMGUVHOn6Rbt24HtN7MzEymTp1KVlYWlZWVTbov4bXXXuPLL78kNjaWJ5980r/NCy64gMjIyP3qf/bZZ7zwwgsAREZG0rVrV1588UWWLl3KMcccA0BZWRm9evU6oNiNMQeuwyQLEal+uHwPEckE/oJz9VMs8Il7SeRiVZ2hqqtFZDawBqd76reqWt2XcRXOlVXxOOc4PiQIGmsBhEJeXh6fffYZq1atQkSoqqpCRLj//vtR1SZdJhpYJ/D+g2uvvZYbb7yRs846i88//5w777yz0XVNnTqVRx99dL/yxMTEpu0Qzo1z06dP5+9//3uTlzHGtFxrJ4tQXg11oar2UdVoVe2vqk+r6lBVHaCqo93XjID696jqIap6mKp+GFC+RFWPdOddE9B11e688cYbXHzxxWzbto2tW7eyY8cOhgwZwpdffsmkSZN44okn8Hqdi8X27HEuJEtOTqaoqMi/jt69e7N27Vp8Ph9vv/22v3zv3r306+ec+3/++edDEv+ECRN4/PHHAeccS2FhIRMmTOCNN94gJyfHH/e2bXWOcGyMCaLISDiEDAZumt8q2+tUd3CH26xZszj33HNrlJ133nm88sorXH755QwcOJCRI0cyatQoXnnlFQCuuOIKTj31VP8J7vvuu48zzjiDk08+mT59+vjXc+edd3LBBRdwwgknNHp+o7keeugh5s+fz4gRIzj66KNZvXo1aWlp3H333UyaNImRI0cyceJEsrKyQrJ9Y8w+IpDBMC554eTW2V47/qHeoPT0dK398KO1a9dyxBFHhCki01z2dzNmf3/7G/z5DqdbOv1oJVjPehORpaqaXrvcWhbGGNMORQQcvVcs9dZfMVjbC/kWjDHGBF3gie1kivjyy9Buz5KFMca0Q5WV+6a7UMjs2aHdniULY4xphzyefdNdKCQuLrTbs2RhjDHtkCULY4wxjQrshkqimFA/ediSRSuLjIxk9OjRHHnkkVxwwQUtGlH2kksu4Y033gDg8ssvZ82aNfXW/fzzz/0D/x2IwYMHs7uOYS0HDx7MiBEjGDVqFJMmTWLXrl11Ln/aaadRUFBwwNs1xjTM4wGfO9bqYLYSosGp/SxZtLL4+HhWrFjBqlWriImJ4Yknnqgxv7kjtj711FOkpaXVO7+5yaIh8+fP5/vvvyc9PZ177723xjxVxefz8cEHH5CSkhLU7RpjqpOFcwh/khkctvSVkG7PkkUYnXDCCWRkZPD5558zfvx4LrroIkaMGEFVVRV/+MMfOOaYYxg5ciRPPvkk4ByAr7nmGtLS0jj99NP9Q2wAnHTSSVTfhPjRRx8xZswYRo0axYQJE9i6dStPPPEE//73vxk9ejQLFy4kNzeX8847j2OOOYZjjjmGRYsWAc74VZMmTeKoo47iyiuvpCk3bY4bN46MjAy2bt3KEUccwdVXX82YMWPYsWNHjZbJCy+84L9D/Ve/+hVAvXEYYxpWWQlV7Bvwc+jmuSHdXqsPUd5mhGuMcpfX6+XDDz9kyhTnKbHffvstq1atYsiQIcycOZOuXbvy3XffUVFRwc9+9jMmTZrE8uXLWb9+PT/88APZ2dmkpaVx6aWX1lhvbm4uv/nNb1iwYAFDhgzxD3U+Y8YMkpKSuOmmmwC46KKLuOGGGxg7dizbt29n8uTJrF27lrvuuouxY8dyxx138P777zNz5sxG9+V///sfI0aMAGD9+vU8++yzPPbYYzXqrF69mnvuuYdFixbRo0cP/9hX1113XZ1xGGMa5vHUTBZSUd5A7ZbrvMkiTMrKyhg9ejTgtCwuu+wyvvrqK4499lj/sOJz585l5cqV/vMRe/fuZePGjSxYsIALL7yQyMhI+vbty8kn7z8mzOLFixk3bpx/XfUNdf7pp5/WOMdRWFhIUVERCxYs4K233gLg9NNPJzU1td59GT9+PJGRkYwcOZK7776bgoICBg0axPHHH79f3c8++4zzzz/fP25VdVz1xZGcnFzvdo0xNbuhALS8HBEYOBDWrYP4+OBur/Mmi3CMUc6+cxa1BQ4Lrqo88sgjTJ48uUadDz74oNFhzJs61LnP5+Prr78mvo5/UU1ZHtjvoUwFBQX1Dm9eX1wNxWGMqV9lZc1kUbzbaVls3w4xMcHfnp2zaIMmT57M448/jse9kHrDhg2UlJQwbtw4Xn31VaqqqsjKymL+/P2HJv7JT37CF198wZYtW4D6hzqfNGlSjWdZVCewcePG8fLLztNuP/zwQ/Lz84OyTxMmTGD27Nnk5eXViKu+OIwxDavdsoinjKm8yiecQqQn+F1SlizaoMsvv5y0tDTGjBnDkUceyZVXXonX6+Xcc89l2LBhjBgxgquuuooTTzxxv2V79uzJzJkz+fnPf86oUaOYOnUqAGeeeSZvv/22/wT3ww8/zJIlSxg5ciRpaWn+q7L+8pe/sGDBAsaMGcPcuXMZOHBgUPZp+PDh3H777Zx44omMGjWKG2+8EaDeOIwxDfN4IIp9AwjGU8ZQMjiFeRAV/E4jG6LctHn2dzNmf+PGwacLY4jB6YFYyQje5Wxu414itXmX4IMNUW6MMR1KZYX6EwVAIiXEUkEFsSHZniULY4xph7TSU+NzKvnEUkGlWLIIio7a7dZR2d/LmHp4aiaLFApIoJSEFEsWLRYXF0deXp4dgNoJVSUvL4+4UA+naUw7pBWVNT5HoPQmG19MaJJFp7rPon///mRmZpKbmxvuUEwTxcXF0b9//3CHYUzbU1m5X1EfssCSRctFR0f772w2xpj2TDx1JwuNqX/UhZboVN1QxhjTUVSf4C4jjuzYAQAcxC40RN22IUsWIvKMiOSIyKqAsm4i8omIbHTfUwPm3SoiGSKyXkQmB5QfLSI/uPMelqaORWGMMR1YhNdpWfyaZ7n9mE8AiKIKYtvfCe7ngCm1ym4B5qnqMGCe+xkRSQOmAcPdZR4TkerhFB8HrgCGua/a6zTGmE6nuhvKQzSTLwwYMLS9JQtVXQDsqVV8NvC8O/08cE5A+auqWqGqW4AM4FgR6QN0UdWv1bmE6YWAZYwxptOqThZvzonh+FP3naeQ9pYs6tFbVbMA3Pdebnk/YEdAvUy3rJ87Xbu8TiJyhYgsEZEldsWTMaYjE697n0VMDEkpUeyli/M5rmMki/rUdR5CGyivk6rOVNV0VU3v2bNn0IIzxpi2pKoKotS9GiomhqQk2IPTFdVRWhbZbtcS7nv1c0EzgQEB9foDO93y/nWUG2NMp+XxQAxusoiOJjoa8sVNFvEdI1nMAaa709OBdwPKp4lIrIgMwTmR/a3bVVUkIse7V0FdHLCMMcZ0Sh4PJFDqfEhIAKAwyk0WIeqGCtlNeSIyCzgJ6CEimcBfgPuA2SJyGbAduABAVVeLyGxgDeAFfqvqH2P3Kpwrq+KBD92XMcZ0Wh6PM8osAElJABRHdwMPRMSH5j6LkCULVb2wnlkT6ql/D3BPHeVLgCODGJoxxrRrHg8kUex8cB9lXBLXDUohooN0QxljjGmhysqAZOG2LErjuwOh64ayZGGMMe1MXS2LioTQnrOwZGGMMe1M9TkLb3Sc/3nbnmT3Lu4OcumsMcaYFqruhqqKS/SXebtYsjDGGBOguhuqKj7JXxbbx5KFMcaYAHUli19d5yaL9jZEuTHGmNCo7obyBSSL5GMOh3vvhTPOCMk2O9WT8owxpiOoPsHtS9h3zoKICLj11pBt01oWxhjTzlR3Q2lCUuOVg8SShTHGtDPV3VCaaMnCGGNMPfw35SVZsjDGGFMP/0CCliyMMaZj8c39FO3bF4qLW7wuT4WPREqQpMTGKweJJQtjjAmxRx+FjMlXI1lZsG1bi9fnKykjAkW6WMvCGGM6hIwMuPZa6E8mAHr11aD1Ph26adzWSUSyJQtjjOkQVOE03ieBMgBkwQKoqGjZSkucBx9FWMvCGGM6Bp8PbuPemoVFRS1bqduyiOxi5yyMMaZDKC+Hr/lJzcIWnuSOKHWTRVdrWRhjTIdQUQHJ1GpJtLBlYcnCGGM6mPJySKGAPaTuK2xhy0JKnXMWliyMMaaDqKiAVPLZEX8Y1x69yClsYcsissxJNnafhTHGdBDl5dCVvQwe1ZXIrslOYQtbFtXJwu7gNsaYDqKiAqLxIHGxRKe6B/eWtizKLVkYY0yHUl7uJIuImChiugenZRFdnSwSEloYXdOFJVmIyA0islpEVonILBGJE5FuIvKJiGx031MD6t8qIhkisl5EJocjZmOMaY6KCojCi8REEdvdaQn4ClvWspDSEkolwXngUStp9WQhIv2A3wHpqnokEAlMA24B5qnqMGCe+xkRSXPnDwemAI+JSGRrx22MMc1RXu4ki8iYKOJTYvEQhTe/ZS2LqoJCyqNarwsKwtcNFQXEi0gUkADsBM4GnnfnPw+c406fDbyqqhWqugXIAI5t3XCNMaZ5qs9ZRMRGk5QsFJOEN79lLYsBe74nJ+WwIEXYNK2eLFT1R+CfwHYgC9irqnOB3qqa5dbJAnq5i/QDdgSsItMt24+IXCEiS0RkSW5ubqh2wRhjmszfsoiNIikJikjGW9D8ZOErLmV4xTKyDhkbxCgbF45uqFSc1sIQoC+QKCK/bGiROsrqHLJRVWeqarqqpvfs2bPlwRpjTAtVn7OIiHGSRTFJ+PY2vxsqf+53ROOlZPTPghhl48LRDXUKsEVVc1XVA7wF/BTIFpE+AO57jls/ExgQsHx/nG4rY4xp86qvhpKYaH/LQltwgrtkwVIAosceF6wQmyQcyWI7cLyIJIiIABOAtcAcYLpbZzrwrjs9B5gmIrEiMgQYBnzbyjEbY0yzVLcsiNrXsmjJpbN7vs1gD6n0Ht4jiFE2LqpVtwao6jci8gawDPACy4GZQBIwW0Quw0koF7j1V4vIbGCNW/+3qlrV2nEbY0xzVLcsqpNFNslIye5mry/vu014OITDDg5ikE3Q6skCQFX/AvylVnEFTiujrvr3APeEOi5jjAk2f8siOtrfsogsDWhZrFwJixbBVVc1ui7fjh+Z4J3L0mFT6dIlhEHXwe7gNsaYEKrdDVVEMpFlAecsxo+Hq6/2P/2uwXU98QwAu9NPDVW49bJkYYwxIVRZVkUEWuOcRXRFQMuistJ5X7268ZUtXMhKRvDjKdMbrxtkliyMMSaEPGVeZyI6mrg4KJZkoj1l4HXLB7gXey5b1vCKvF5ilnzFAsaRkhKycOtlycIYY0LIW+4mhagoRMAT4w7TUd3tlOoOg/f66w2vaPlyIstKWMgJdO0amlgbYsnCGGNCqKrc40xEOdcTeeLckWfdYcpLdxU6nwsKGlzPWzcsBGAhJ4SlZRGWq6GMMaaz8AS0LACq4pNgL1BczPvvw8jNBSSAc41tXdatg/feI37JIjI4hCz60q1ba0Rek7UsjDEmhGJK9zoTbt+RL9FpWXjzi/jDH5xHrgL1J4vHHoM//pETK+aSPfBYXn4ZhgwJddT7s2RhjDEhlFya7Uz0csZG9SalAPDp85lsXOshCffcRT3JomrpcgASKCNmQG8uuiik4dbLkoUxxoRQl3J3mDs3WezocywFEansef5/TDneaXVUEIPu2QMaMEbqBx9AVhbeJSv8Rdqte2uFvR9LFsYYE0JdK2omi7guMez0HUR0eSF3zHDmLeVopLwcst1WiNcLp58OffsSW1mMF+d5b9LDkoUxxnRIKR43WbiPTUhKgnLiiKOcQRUbAJjLJKfO1q3Oe1HNUWnf4HwAEvqH4cy2y5KFMcaEiCqkVuZQFtsVYmMBJ1mUEU88ZcRtWw/Ax0x2FtiyxXkvLPSvw0MUW3/+ezQykrRzDm3V+ANZsjDGmBDJzoYemkNFl17+ssCWRfTWDXi692YlI52ZdSSL1Qwn5mfHIAUFyJijWjP8GixZGGNMiGzbBr3Ioapnb39ZYMsievN6fEMPo5RESomHfOcy2vnv7PXXX85RDB3qLhhGliyMMSZEtm51kkVU37pbFpEZ64kafhgAJST6hwD5xx37WhZLOZphw1o17DpZsjDGmBCpblnED6qZLMqIpy87kd27iUw7jLPPhtKIJH+y6IqTLF465iGe4bKw3IRXmw33YYwxIbJnzS56kQvDBvvLkpMhnzhSKXAKDjuMoVlQoon+x612iyoELzyecx69BsUTF9f6sdfWpJaFiBwqIvNEZJX7eaSI/Cm0oRljTPvWc/lcZ2LiRH9ZdcvC79BDSUiAQk1C3ZZFaqTTsli5rYtzvqINaGo31H+BWwEPgKquBKaFKihjjOkIDtv6MQWxvWD0aH9ZQoJzzgJwBhccMoT4eOechebuhgsvJJ3v8CGUkNhmkkVTu6ESVPVbEQks84YgHmOM6RC0ysfxhXPJOHwK6RH7fpfHxgYki0MOgehof7KIWPYZLFvKOcBmhqBEhGU48ro0tWWxW0QOARRARM4HskIWlTHGtHN71/xID3az98if1SiPjQ3ohjrMuRIqPh4qiK1Rb2PkEVxxBVx7bauE26imtix+C8wEDheRH4EtwC9DFpUxxrRz+etzSAGiB/apUR4TE9CyONS5IzshASqJqVEvYczhPPlkKwTaRE1KFqq6GThFRBKBCFUtamwZY4zpzIo3O2NCxQ3qXaO8vpZFYUDLopJo8oYe1zqBNlFTr4a6V0RSVLVEVYtEJFVE7g51cMYY015VbHdGkE0+pFeN8hoti4BkUd2y8B7Unz5kseuEC1ov2CZo6jmLU1W1oPqDquYDpzV3oyKSIiJviMg6EVkrIj8RkW4i8omIbHTfUwPq3yoiGSKyXkQmN3e7xhjTWrw7nZZFyqH7J4svOJFZTIOjjwZqdkPtiurHHrozcZLQljQ1WUSKiL+NJCLxUOtszIF5CPhIVQ8HRgFrgVuAeao6DJjnfkZE0nAu0x0OTAEeE5HIFmzbGGNCTndlU0IC3QfVHNMpNha2MoSLmOVkCWqe4P4hrx8TJzoXSrUlTU0WLwHzROQyEbkU+AR4vjkbFJEuwDjgaQBVrXRbLWcHrPN54Bx3+mzgVVWtUNUtQAZwbHO2bYwxrSVyTw67I3oRU/O89X6foWY31Mayflx6aSsEeICalCxU9X7gHuAInF/4f3PLmuNgIBd4VkSWi8hT7onz3qqa5W4vC6huu/UDdgQsn+mW7UdErhCRJSKyJDc3t5nhGWNMy8UVZFMQ3Wu/8tg6+mTi4yGGSgCypQ9TpoQ6ugPX5LGhVPVD4MMgbXMMcK2qfiMiD+F2OdWjro47raMMVZ2Jc4kv6enpddYxxpjWkFCSw66EgfuVR9bRiZ6QAIk4Q30cNCi2zdyIF6jBloWIfOm+F4lIYcCrSEQKG1q2AZlApqp+435+Ayd5ZItIH3d7fYCcgPoDApbvD+xs5raNMaZVdCnPoSx5/5ZFXeLj4ROc8aNkeFoow2q2BpOFqo5135NVtUvAK1lVuzRng6q6C9ghIoe5RROANcAcYLpbNh14152eA0wTkVgRGQIMA75tzraNMaZV+Hx08+bgSW16sniHczmYTYy7tw32QdGEbigRiQBWquqRQdzutcDLIhIDbAZ+jZO4ZovIZcB24AIAVV0tIrNxEooX+K2qVgUxFmOMCSpfUQlRVCHdUhuvDP4hyLdwMCNHhjCwFmg0WaiqT0S+F5GBqro9GBtV1RVAeh2zJtRT/x6cE+zGGNPmFe+ppAsQ27VpD6KIiIDXX4fj2tZN2zU09QR3H2C1iHwL7lkYQFXPCklUxhjTjhXnVdAFiE6s4zpZYOxYOOGEmmXnnx/6uFqiqcnirpBGYYwxHUhpgXMZbExS3cli4cLWjCY4GkwWIhIHzACGAj8AT6uqPcfCGGMa0FiyaI8auynveZxzCz8ApwL/CnlExhjTzlUni9guLRkVqW1prBsqTVVHAIjI09glq8YY06iyggoAYrt0npaFp3rCup+MMaZpygqdlkV8B0oWjbUsRgXcqS1AvPtZAG3ujXnGGNORVbjJIq5rJ+mGUlUbCtwYYw5QRZGTLBJSOk7LoqlDlBtjjGmiyiL3nEWyJQtjjDH1qCx2WhYS13G6oSxZGGNMkHlKnGRR55OO2ilLFsYYE2TeUksWxhhjGuEpds5Z1PlYvHbKkoUxxgRZeaG1LIwxxjSi+tJZSxbGGGPq5PPtuxrKkoUxxpiaVOHXv6bo3c+IVvecRQdKFk19noUxxpj6LF0KgwfDc8/hK4khhp74IiKJiOw4g2BYy8IYY1ri448hPZ3cvz8FgGxYTwyVaHTHaVWAJQtjjGm23/0Ott75LAA9/3ULALHbnGRBTMe5bBYsWRhjTLO9+XQBBy1+p0ZZfMEuepILsdayMMaYTs/jgdNKXyeOiv3mHckqIuIsWRhjTKe3dy9czAusi0jjI6bUmDeCH5AOdPc2WLIwxphmKSiAo1hO6dhJlA1PB2DjEWdRRQQRaIe6bBbCmCxEJFJElovI/9zP3UTkExHZ6L6nBtS9VUQyRGS9iEwOV8zGGFNtb24lSZSQMLAH5y79E/zjHwxb8ToyeJBTwZJF0FwHrA34fAswT1WHAfPcz4hIGjANGA5MAR4TkY5z8bIxpl0qycwHIKpnqjNg4E03QUwMEUcc7lQYMiSM0QVfWJKFiPQHTgeeCig+G3jenX4eOCeg/FVVrVDVLUAGcGwrhWqMMXUq+3EPADEHdas548EH4Z13YPbsVo8plMJ1B/eDwB+B5ICy3qqaBaCqWSLSyy3vBywOqJfplu1HRK4ArgAYOHBgkEM2xph9Knc5ySKuX61kceihzquDafWWhYicAeSo6tKmLlJHmdZVUVVnqmq6qqb37Nmz2TEaY0xjPDlOskjs362Rmh1DOFoWPwPOEpHTgDigi4i8BGSLSB+3VdEHyHHrZwIDApbvD+xs1YiNMaYW3e0ki4ROkixavWWhqreqan9VHYxz4vozVf0lMAeY7labDrzrTs8BpolIrIgMAYYB37Zy2MYYU1O+c4JbuneOZNGWRp29D5gtIpcB24ELAFR1tYjMBtYAXuC3qloVvjCNMQYi9u7BhxDRpUu4Q2kVYU0Wqvo58Lk7nQdMqKfePcA9rRaYMcY0IrpwD4WRqaREdI57mzvHXhpjTJDFlu6hOKZzdEGBJQtjjGmW+LI9lMZZsjDGGNOApIo9VCRYsjDGGNOAJG8+lcmWLIwxxtTD64VU3UNVl9TGK3cQliyMMeYAFRb4SCUfTe08LYu2dJ+FMca0fXv2EHH7/xGBEtFJbsgDSxbGGHNgbrqJlGefBSCyZ+dJFtYNZYwxB6KgwD+53/DkHZglC2OMaaKSEvhhaYX/c+xBdoLbGGNMLa/euY607R/5PycMsJaFMcaYWpLnvUMkvn2fB1qyMMYYU0tJTgk+hOwb7gOgy2BLFsYYY2opySvHGxVH7wduBlWIjg53SK3GkoUxxjSBxwOUl+GNjg93KGFhycIYY5qgoADiKcMXExfuUMLCkoUxxjRBQQHEUY7GWsvCGGNMPapbFhpvycIYY0w9qpOFJFiyMMYYU4/qZBGRYOcsjDHG1KP6nEVkkrUsjDHG1CM/32lZRCVbsjDGGFOP6m4oa1kYY4ypV0EBJESUI3F2zqJViMgAEZkvImtFZLWIXOeWdxORT0Rko/ueGrDMrSKSISLrRWRya8dsjDEFBZBAGdils63GC/xeVY8Ajgd+KyJpwC3APFUdBsxzP+POmwYMB6YAj4lIZBjiNsZ0Ylk7lThLFq1HVbNUdZk7XQSsBfoBZwPPu9WeB85xp88GXlXVClXdAmQAx7Zq0MaYTm3hIyt4/4tEEnwlMHhwuMMJi7CesxCRwcBRwDdAb1XNAiehAL3cav2AHQGLZbplda3vChFZIiJLcnNzQxa3MaZz2fHi5yRQxu3cDVddFe5wwiJsyUJEkoA3getVtbChqnWUaV0VVXWmqqaranrPnj2DEaYxpqPbtAm2bm2wStKOteymO6Nn3w5RUa0TVxsTlmQhItE4ieJlVX3LLc4WkT7u/D5AjlueCQwIWLw/sLO1YjXGdFyqwNChMGQIOTl11/F4IDV7HYV9D+eCC1o1vDYlHFdDCfA0sFZVHwiYNQeY7k5PB94NKJ8mIrEiMgQYBnzbWvEaYzquF17YN33zzXXXWbYMDtV1cPgRrRNUGxWO9tTPgF8BP4jICrfsNuA+YLaIXAZsBy4AUNXVIjIbWINzJdVvVbWq1aM2xnQ4a9fum95Zu79ClZJi5dYrC/iMHIrGHd6qsbU1rZ4sVPVL6j4PATChnmXuAe4JWVDGmE6pa9d90712rgBGA6APPoTccD2JgJcFACSnd+5kYXdwG2M6rZSUfdMvrjoKXfE9AJ6HH/OXvx9/HsTGwpgxrRxd22LJwhjTaVVW1vy891PndKgnct+QHok94mHBAujTpzVDa3MsWRhjOq0dO2p+LvlyBQCFm/bdpxWxdQsca/cBW7IwxnRa27ftu2WrgK5ErlsNQKEmA5D/9FsQYYdJsGRhjOnE8rYVA+C5537e53Ris7excyd4iGb1oeeSeum5YY6w7bBkYYzptPZu3wtAdI+u5CUOInlvJsuXVNGD3XQ/vEeYo2tbLFkYYzqlykooz3aSBV27UtpzEFHqZcDjt3EQ2cT16x7eANsYSxbGmE5pwScVXMkTzoeUFHwDBgEw8qP7AYgfYC2LQJYsjDGd0kH/vplredT5kJpKzLBBNebH9rGWRaDOOXyiMabT67Fq/r4PY8bQZXh5rQrWsghkLQtjTKdTUqzE5PzofFi6FKKi6DsssWalTn4TXm2WLIwxnYsq30+8iW6ax6Yb/+MfxmPQ4FpD1h3euceCqs2ShTEmdKqqYNEi8PnCHYlf3off8tPFztMRDvntFH/5oMBTFnfcAYm1WhqdnCULYzq4H/48mx3dRpJx0R1OQUkJfPwxqLLt6U/Zc/DR8L//NX2F27e7Tw3an7fSx6r/fOE8MQgoev0jGDsWHnigzvp+33wDGRlNj6GZVOGLi58G4Mdf3gwHH+yf16VLQMU//znksbQ7qtohX0cffbQa09lt+WKblhCv6hwnteTiGfrdqMtUQTOTD1cPkaqgX0aM1WWHTdOih5/2L5s1/Y/qvfxKXbu4QHdOuliLJpytOmOGKqjvnnvr3N47J/7Lvy197z29lodUQcvHT3Yq+Hyq8+erJzdf169X/fK/a/SDw65TBfX26KVZ51ypRcOPVfV4gvo9bFpTrt8lnKCz5f/pHlJ0w/G/qrtideydGLBE6zimhv2gHqqXJYtOYO/ecEfQ5lRmbNMdF9ygm6dcpbkJA/QbOVZLidO3p72670DovrxE6Jq+E/TRrrf6Pyvo53GTdHXiMf56WxmolUT5PxfQRUsiErXw+bdU1Tn+r3piob504f/0K47fbzsKWhGdoPrhh6oTJ6qCzj3udn2KS+usW/1a9Yt7tWr3nhZ/J1u3ql7b/eUa6658+391V7ZkYcnCdCwb3l2jVZFRWnrj7UFdr2/9Bt161X26afJVmrMm1yn0ep33LVtUi4sPfKWFhc4v8m3bghZntT2b83XTtf9W3b1bKypUF/U9f7+D7pLJt2lpqeq5vb5Ur0Rq7sU3qFZUqO7aper1an6+avHa7br4zUzdS3KNZVdLmv7Q+2Rdce/7+sDxr+nD3e/Um6du0WWMVgUtSh2w3/ZKU/toLt01j1R/2Q76+ad90dH+6W1T/6A/vvGVfvHAElXQdRyqKzlyX5IhWpcfPlWLx01xypYuPeDv6JTRuZon3Zzl8/JUP/rIyXB1mTFDdezYFv1N2jtLFqbNqqpSLd+2SzUzc/+Zu3ernn+++n5/k2pFhWa884Pmx/SscXD6scdIXf+LO3XrU59oWVxX/fEXN6nu2NG0jW/cqLvnLNK1tz6v2446298tU/v11Sl/1ioitHDoaCdpzJmjeuqpzsHH61Vf1q661+/zaflvb1AFzet1mJbmldZdz+tVLS9vWsyqWlrk1S/OeUC/5jhV0MKoFL075R9aTozO63ORLrn3Y83bXqzf3/WW+soC1ltW1uB6t320RneNPU+rVq5S39ZtWlpPuNMvqtTfycO6DSdZvBB1iWa8uEj1vfdUs7M1K0u1IKdCdw1M17zb/6VXH/udKuid3KG5dFcF9URE1zhof//uFn395Qo9+YRKnXf9HH3vT4v1rcHXazkxNf4WO9PPUM3N9S/344Zi/W7AObrk0Av9id370Sdaetk1uuz+T/Yt+/vfN/n77cwsWZi2ZedO1aIizf9/v9FlSWP9/6HLjjtRyx58XHd9vVl17VqtGHCwf97erv31xwjnF+rmuMP1tXNe0a85Xhfx0/0O7iUpfbTq/n9q5e13qu+DD7Xgp1O0dOKZ+36Zer1aetf9NZYpIlE/4yR9iYt0N930tZhf1Jk46ntt7DtOPct/2LePlZW684zLa9Qpju6qOSlDdVfSIbp47O919sNZumTwz/0Hz6yDf6Le+/+pvgUL1XfnXaoff6yan+8/qPqqfPrpfd/pnISp/nUuYKwuj3e6fyqTUlQ3bgz5n2/TJtXevVV/f51HS3Iab235fKolOwv0m8U+fTbxalXQjDtfbHQ5j0f18b/n6xP35ulNh7ylz3CJ8++kW1/N/usTuq7/ybo+4rAa3/EXU+7VCqJrlM27fV4wdrtTsGRhQi8vT7WgQLWyUote/1D3/vc1p8N41qya9d54Y78D7UO97tbbuLtGWUFsT91FLz2Td/VyZuonTNBvEk7U1f+Z719VQYHzg/yjK97UNWnn6dqrHtIrj1mqu+hV70F9b3zvGp9fGXKbLn5osa5e6a3xwzs7W/Xsvt84B+G4JP0x06dXdn/dv9yzTK/xq7eEePUSoVl9Rmv+ax/7zwE8kPgnnfepT2ee9rbm07XuRBN5mD6VdF2N7prarxV9p2hG/HD/502X3aP+n/5VVaorVzavm6yZ6uvJaZTX26wuucJC1T/+UfWklOW6mcH7vvfIZM28+xldO2Ci7o3oqgq6XQboP7lRN3VP1+IH/9vMQDsnSxam5fbuVf3uu/3Ly8o0/8159R7kFJyWREWFev50p3OgY4gWkqSVRGnm97vV61VdsUL1sRH/8S+zLWKQ3j19g+bkOMfA3bubdoAqLVX90+9L9a837NHxI3L1ytTX9M2L39b7rs3Uu+Pv1rkRk1VBv0o9Tb94aHmD6/L5VEtenePv1qqoUP3fS/n69z+X6MKF++Lx+VRf+neOPslvauz3gol/1ZKSfXE982CBbnr4Pa0s9eiGmfN1y89vVM/Cr/3b+/vdXr3lkNk6v/8v9bazV+lf+z6u22WgKmg2PfXbxJP0s4tmqmfz9qb8xTqkHTtUb+7ymCronl9dq4F9ZVVen+au262VpcG9mqozsWRhWu70051/MnfdpVpWpgXX/VlXHn2Jft/lZzUOkEu7nKQvT31H/zHmFc3C+RW/+9DjNT++j7/ObZO+08/n7K2/D7+qyjkyB0FggvH5nK6NrC0N990319q1qvMYrwr6w60vBWWdJXvK9b1ZRZqb09yf8h1Pzk6PFj49O+iX2Jr6k4U48zqe9PR0XbJkSbjD6FB06FBk06Y653007FoGXH8ew68aB7Jv2IQvv4RvJ97OjeX38knkZH446XccMjWdMy/r1WGfVvnRmyX0KtrEmEtGhjsUYw6YiCxV1fT9yttLshCRKcBDQCTwlKre11D9sCQL1RoHynavstIZZK28nMrNmcRcfjGPcA1DyeBUPuKO/s9w6dxpDN75lXOXbmxsnavJzVF++CyX487sZSMoGNPG1Zcs2sUQ5SISCfwHmAhkAt+JyBxVXRP0jT39NOTngypVeQUULN9M1LbNxPlKkMQEIlK6ENk9BenaFVJSIDkZSkthwwZYsQImTEB37QKforFxaEIiumcPGhGJJiUT3a0LkpgA5eX4oqLx+KLwbtkO2dlEZe+kKj6J6MI8vGkjieiSiMbE4skvJmLrZnwVHqq8SmzRbnzJKUikEF2cT2RFKRWjjqOqpJyovF34EpIQAU1MhqhIiI0lwlvpvEdGUOWpoqqkAl9RMRGFBYjXi3g9SJUXqfIQ4fMSqV7EU0mEOmP6xAB76YLvrHPp+eeT8YyCv0a739kRExr8Snv2Ek6e1ivofypjTOtpF8kCOBbIUNXNACLyKnA2EPRkse26fzGoZC0APqLIZxCbOZgi+pNAKV0oJIUsUqWArlpAIqWUE0sMlUSgFDzzJhkMRRHiKCeJYopIpopIkimiC4UkU0QJiUTjIYZKdtKfIpLJ4RCSKWIvfei3awdxlBNLBVVEspYjiMBHCYnsoRspRQUA5JNKX3ZyxNdrKSWBUnqRQy+i8TCQ7eTRnTj2EoEPLxVEUkUVkVQSQwXd2cMwyojHSxQeomu8VxDLGtI4J34uO446k/5XncV1v4wM9ldujGkH2kuy6AfsCPicCRwXig09ffUysjM9VGkEyT1i+ckJUfToAWU7IacIioudV1GRM5BmtHghKorISIgQJSrCR0R0JBEREBnpvKqnvV7IyXHeY2OdQS0TEyE1FRISIC4OPFFOQ2WJux0R6N4dunVz3g/tBjExsGMHVFSA7IasfMiPcxo6MTEQ4YVyD6zxOtvyeJz36um4uH3rS0uF+HhnudhY55WfDwUFTtkNgyA5eVoovmpjTDvSXpJFXScC9jvZIiJXAFcADBw4sFkb+uv9cUDcASwR+BUKzimV0Gvm7jVJamro1m2MaZ/ay/UomcCAgM/9gZ21K6nqTFVNV9X0nj17tlpwxhjT0bWXZPEdMExEhohIDDANmBPmmIwxptNoF91QquoVkWuAj3H6eZ5R1dVhDssYYzqNdpEsAFT1A+CDcMdhjDGdUXvphjLGGBNGliyMMcY0ypKFMcaYRlmyMMYY06h2M5DggRKRXGBbuONooR7A7nAHEQK2X+2L7Vf70tL9GqSq+92o1mGTRUcgIkvqGv2xvbP9al9sv9qXUO2XdUMZY4xplCULY4wxjbJk0bbNDHcAIWL71b7YfrUvIdkvO2dhjDGmUdayMMYY0yhLFsYYYxplyaIVicgAEZkvImtFZLWIXOeWdxORT0Rko/ue6pZ3d+sXi8ijAetJEJH3RWSdu577wrVPbjxB2a9a65wjIqtacz/qiCFo+yUiMSIyU0Q2uH+388KxT24swdyvC0XkBxFZKSIfiUiPcOyTG8uB7tdEEVnqxr9URE4OWNfRbnmGiDwsInU9gK1d7VeLjxuqaq9WegF9gDHudDKwAUgD7gducctvAf7PnU4ExgIzgEcD1pMAjHenY4CFwKntfb8C1vdz4BVgVUf4e7nz7gLudqcjgB7tfb9wRq3Oqd4Xd/k729F+HQX0daePBH4MWNe3wE9wHn/5YTv7/1XnfrX0uBGWnbeX/x/Bu8BEYD3QJ+Afxvpa9S6p66AaMP8h4Dfh3p9g7BeQBHzp/mcIa7II8n7tABLDvQ/B3C8gGsgFBrkH1SeAK8K9Pwe6X265AHlArFtnXcC8C4Enw70/Ld2vOuYd0HHDuqHCREQG4/wC+AborapZAO57rwNYTwpwJjAv+FEeuCDs19+AfwGloYqxOVqyX+7fCOBvIrJMRF4Xkd4hDLfJWrJfquoBrgJ+wHnMcRrwdCjjbapm7Nd5wHJVrQD64TzKuVqmWxZ2LdyvwPWkcIDHDUsWYSAiScCbwPWqWtiC9UQBs4CHVXVzsOJrQTwt2i8RGQ0MVdW3gx1bSwTh7xWF89z4Rao6Bvga+GcQQ2yWIPy9onGSxVFAX2AlcGtQg2yGA90vERkO/B9wZXVRHdXCfo9BEParurxZxw1LFq3M/Q/2JvCyqr7lFmeLSB93fh+cfuCmmAlsVNUHgx7oAQrSfv0EOFpEtuJ0RR0qIp+HJuKmCdJ+5eG0lKqT4OvAmBCE22RB2q/RAKq6SZ1+jdnAT0MTcdMc6H6JSH+cv8vFqrrJLc7ESe7V+uO0nMImSPtVrVnHDUsWrci9ouJpYK2qPhAwaw4w3Z2ejtMn2di67ga6AtcHOcwDFqz9UtXHVbWvqg7GOaG6QVVPCn7ETRPE/VLgPeAkt2gCsCaowR6AIP47/BFIE5HqEUonAmuDGeuBOND9crti3gduVdVF1ZXdLp0iETneXefFNOH/ZKgEa7/cec0/boT7ZE1neuEcABWnub7CfZ0GdMfpO9zovncLWGYrsAcoxvnFk4bzS0dx/mNWr+fy9r5ftdY5mPBfDRW0/cI5CbzAXdc8YGAH2a8Z7r/DlTgJsXt72S/gT0BJQN0VQC93XjqwCtgEPIo72kV73i9aeNyw4T6MMcY0yrqhjDHGNMqShTHGmEZZsjDGGNMoSxbGGGMaZcnCGGNMo6LCHYAxHYGIVOEMexENeIHngQdV1RfWwIwJEksWxgRHmaqOBhCRXjij5nYF/hLOoIwJFuuGMibIVDUHuAK4RhyDRWShO4jgMhH5KYCIvCgiZ1cvJyIvi8hZIjJcRL4VkRXucyKGhWtfjKlmN+UZEwQiUqyqSbXK8oHDgSLAp6rl7oF/lqqmi8iJwA2qeo6IdMW5o3YY8G9gsaq+LCIxQKSqlrXqDhlTi3VDGRM61aOXRgOPuqPqVgGHAqjqFyLyH7fb6ufAm6rqFZGvgdvdweDeUtWNYYjdmBqsG8qYEBCRg3ESQw5wA5ANjMIZcygmoOqLwC+AXwPPAqjqK8BZQBnwceDjPo0JF0sWxgSZOwrrEzhPlVOcE91Z7pVRvwIiA6o/hzsCqKqudpc/GNisqg/jjCw6stWCN6Ye1g1lTHDEi8gK9l06+yJQPZz0Y8CbInIBMB9nRFAAVDVbRNYC7wSsayrwSxHxALuAv4Y8emMaYSe4jQkjEUnAuT9jjKruDXc8xtTHuqGMCRMROQVYBzxiicK0ddayMMYY0yhrWRhjjGmUJQtjjDGNsmRhjDGmUZYsjDHGNMqShTHGmEb9f4LaVzAEVq6dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
